{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/online1/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # SGD optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# data : shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train_samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "RESHAPED = 28*28\n",
    "X_train = x_train.reshape(60000, RESHAPED)\n",
    "X_test = x_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalize \n",
    "X_train /= 255 # image max intensity is 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train_samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# vonvert class vectors to binary class matrics\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model compilation\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 1.3778 - acc: 0.6692 - val_loss: 0.8900 - val_acc: 0.8296\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.7886 - acc: 0.8293 - val_loss: 0.6538 - val_acc: 0.8582\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.6403 - acc: 0.8506 - val_loss: 0.5596 - val_acc: 0.8698\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.5688 - acc: 0.8611 - val_loss: 0.5068 - val_acc: 0.8788\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.5251 - acc: 0.8684 - val_loss: 0.4733 - val_acc: 0.8833\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.4952 - acc: 0.8727 - val_loss: 0.4493 - val_acc: 0.8864\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4730 - acc: 0.8768 - val_loss: 0.4314 - val_acc: 0.8885\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.4558 - acc: 0.8799 - val_loss: 0.4172 - val_acc: 0.8917\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4419 - acc: 0.8831 - val_loss: 0.4058 - val_acc: 0.8940\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4305 - acc: 0.8850 - val_loss: 0.3964 - val_acc: 0.8968\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4207 - acc: 0.8869 - val_loss: 0.3882 - val_acc: 0.8984\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4123 - acc: 0.8893 - val_loss: 0.3814 - val_acc: 0.8989\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4051 - acc: 0.8905 - val_loss: 0.3754 - val_acc: 0.8998\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3986 - acc: 0.8915 - val_loss: 0.3701 - val_acc: 0.9012\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3929 - acc: 0.8928 - val_loss: 0.3654 - val_acc: 0.9019\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3877 - acc: 0.8945 - val_loss: 0.3611 - val_acc: 0.9028\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3830 - acc: 0.8955 - val_loss: 0.3573 - val_acc: 0.9036\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3788 - acc: 0.8962 - val_loss: 0.3537 - val_acc: 0.9044\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3748 - acc: 0.8970 - val_loss: 0.3505 - val_acc: 0.9047\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3712 - acc: 0.8982 - val_loss: 0.3476 - val_acc: 0.9063\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3679 - acc: 0.8988 - val_loss: 0.3449 - val_acc: 0.9067\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3649 - acc: 0.8995 - val_loss: 0.3423 - val_acc: 0.9074\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3619 - acc: 0.9000 - val_loss: 0.3399 - val_acc: 0.9077\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3592 - acc: 0.9008 - val_loss: 0.3378 - val_acc: 0.9079\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3567 - acc: 0.9012 - val_loss: 0.3357 - val_acc: 0.9083\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3543 - acc: 0.9016 - val_loss: 0.3338 - val_acc: 0.9090\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3521 - acc: 0.9024 - val_loss: 0.3319 - val_acc: 0.9093\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3500 - acc: 0.9026 - val_loss: 0.3302 - val_acc: 0.9091\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3480 - acc: 0.9032 - val_loss: 0.3286 - val_acc: 0.9093\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3460 - acc: 0.9037 - val_loss: 0.3270 - val_acc: 0.9105\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3442 - acc: 0.9040 - val_loss: 0.3255 - val_acc: 0.9108\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3425 - acc: 0.9048 - val_loss: 0.3243 - val_acc: 0.9107\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3408 - acc: 0.9048 - val_loss: 0.3228 - val_acc: 0.9117\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3393 - acc: 0.9057 - val_loss: 0.3216 - val_acc: 0.9121\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3378 - acc: 0.9060 - val_loss: 0.3204 - val_acc: 0.9122\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3363 - acc: 0.9064 - val_loss: 0.3194 - val_acc: 0.9125\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3349 - acc: 0.9065 - val_loss: 0.3181 - val_acc: 0.9131\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3336 - acc: 0.9074 - val_loss: 0.3171 - val_acc: 0.9132\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3324 - acc: 0.9075 - val_loss: 0.3160 - val_acc: 0.9135\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3311 - acc: 0.9081 - val_loss: 0.3151 - val_acc: 0.9139\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.3299 - acc: 0.9079 - val_loss: 0.3141 - val_acc: 0.9143\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.3288 - acc: 0.9088 - val_loss: 0.3133 - val_acc: 0.9143\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.3277 - acc: 0.9091 - val_loss: 0.3123 - val_acc: 0.9144\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3266 - acc: 0.9092 - val_loss: 0.3116 - val_acc: 0.9146\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3256 - acc: 0.9096 - val_loss: 0.3108 - val_acc: 0.9149\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3246 - acc: 0.9097 - val_loss: 0.3099 - val_acc: 0.9149\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3236 - acc: 0.9100 - val_loss: 0.3092 - val_acc: 0.9147\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3227 - acc: 0.9102 - val_loss: 0.3084 - val_acc: 0.9149\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3217 - acc: 0.9109 - val_loss: 0.3077 - val_acc: 0.9159\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3209 - acc: 0.9109 - val_loss: 0.3070 - val_acc: 0.9157\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3200 - acc: 0.9114 - val_loss: 0.3064 - val_acc: 0.9156\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3192 - acc: 0.9116 - val_loss: 0.3057 - val_acc: 0.9160\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.3184 - acc: 0.9118 - val_loss: 0.3050 - val_acc: 0.9159\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3176 - acc: 0.9119 - val_loss: 0.3044 - val_acc: 0.9156\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3168 - acc: 0.9117 - val_loss: 0.3039 - val_acc: 0.9159\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3161 - acc: 0.9122 - val_loss: 0.3032 - val_acc: 0.9159\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3154 - acc: 0.9129 - val_loss: 0.3027 - val_acc: 0.9162\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3146 - acc: 0.9131 - val_loss: 0.3022 - val_acc: 0.9162\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3140 - acc: 0.9128 - val_loss: 0.3016 - val_acc: 0.9164\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3133 - acc: 0.9133 - val_loss: 0.3011 - val_acc: 0.9164\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3126 - acc: 0.9132 - val_loss: 0.3007 - val_acc: 0.9170\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3119 - acc: 0.9134 - val_loss: 0.3001 - val_acc: 0.9167\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3114 - acc: 0.9136 - val_loss: 0.2996 - val_acc: 0.9173\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3107 - acc: 0.9140 - val_loss: 0.2992 - val_acc: 0.9174\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3101 - acc: 0.9140 - val_loss: 0.2987 - val_acc: 0.9169\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3095 - acc: 0.9143 - val_loss: 0.2984 - val_acc: 0.9175\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3090 - acc: 0.9145 - val_loss: 0.2978 - val_acc: 0.9175\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3084 - acc: 0.9144 - val_loss: 0.2975 - val_acc: 0.9177\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3079 - acc: 0.9145 - val_loss: 0.2970 - val_acc: 0.9177\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3074 - acc: 0.9148 - val_loss: 0.2966 - val_acc: 0.9175\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3068 - acc: 0.9150 - val_loss: 0.2962 - val_acc: 0.9175\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3063 - acc: 0.9150 - val_loss: 0.2959 - val_acc: 0.9177\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3058 - acc: 0.9150 - val_loss: 0.2955 - val_acc: 0.9178\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3053 - acc: 0.9154 - val_loss: 0.2951 - val_acc: 0.9175\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3048 - acc: 0.9156 - val_loss: 0.2947 - val_acc: 0.9175\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3043 - acc: 0.9159 - val_loss: 0.2944 - val_acc: 0.9177\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3039 - acc: 0.9156 - val_loss: 0.2940 - val_acc: 0.9182\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3034 - acc: 0.9159 - val_loss: 0.2937 - val_acc: 0.9183\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3030 - acc: 0.9162 - val_loss: 0.2934 - val_acc: 0.9182\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3025 - acc: 0.9162 - val_loss: 0.2930 - val_acc: 0.9182\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3021 - acc: 0.9164 - val_loss: 0.2927 - val_acc: 0.9186\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3016 - acc: 0.9165 - val_loss: 0.2925 - val_acc: 0.9187\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3013 - acc: 0.9166 - val_loss: 0.2921 - val_acc: 0.9183\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3008 - acc: 0.9163 - val_loss: 0.2918 - val_acc: 0.9182\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3004 - acc: 0.9164 - val_loss: 0.2915 - val_acc: 0.9186\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3000 - acc: 0.9168 - val_loss: 0.2913 - val_acc: 0.9187\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2997 - acc: 0.9167 - val_loss: 0.2910 - val_acc: 0.9187\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2993 - acc: 0.9170 - val_loss: 0.2906 - val_acc: 0.9192\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2989 - acc: 0.9171 - val_loss: 0.2903 - val_acc: 0.9191\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2985 - acc: 0.9172 - val_loss: 0.2901 - val_acc: 0.9193\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2982 - acc: 0.9170 - val_loss: 0.2898 - val_acc: 0.9195\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2978 - acc: 0.9174 - val_loss: 0.2896 - val_acc: 0.9195\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2975 - acc: 0.9173 - val_loss: 0.2894 - val_acc: 0.9191\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2971 - acc: 0.9172 - val_loss: 0.2891 - val_acc: 0.9193\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2967 - acc: 0.9175 - val_loss: 0.2889 - val_acc: 0.9194\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2964 - acc: 0.9175 - val_loss: 0.2886 - val_acc: 0.9197\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2961 - acc: 0.9177 - val_loss: 0.2884 - val_acc: 0.9195\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2958 - acc: 0.9178 - val_loss: 0.2882 - val_acc: 0.9194\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2954 - acc: 0.9181 - val_loss: 0.2880 - val_acc: 0.9196\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2951 - acc: 0.9180 - val_loss: 0.2878 - val_acc: 0.9197\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2947 - acc: 0.9178 - val_loss: 0.2875 - val_acc: 0.9203\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2945 - acc: 0.9182 - val_loss: 0.2873 - val_acc: 0.9194\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2942 - acc: 0.9184 - val_loss: 0.2870 - val_acc: 0.9203\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2939 - acc: 0.9185 - val_loss: 0.2868 - val_acc: 0.9203\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2936 - acc: 0.9184 - val_loss: 0.2866 - val_acc: 0.9203\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2933 - acc: 0.9185 - val_loss: 0.2864 - val_acc: 0.9208\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2930 - acc: 0.9186 - val_loss: 0.2862 - val_acc: 0.9209\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2927 - acc: 0.9186 - val_loss: 0.2861 - val_acc: 0.9205\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2924 - acc: 0.9187 - val_loss: 0.2858 - val_acc: 0.9209\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2922 - acc: 0.9187 - val_loss: 0.2856 - val_acc: 0.9207\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2918 - acc: 0.9189 - val_loss: 0.2855 - val_acc: 0.9204\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2916 - acc: 0.9190 - val_loss: 0.2852 - val_acc: 0.9205\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2913 - acc: 0.9190 - val_loss: 0.2850 - val_acc: 0.9212\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2911 - acc: 0.9191 - val_loss: 0.2849 - val_acc: 0.9212\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2908 - acc: 0.9192 - val_loss: 0.2846 - val_acc: 0.9212\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2906 - acc: 0.9193 - val_loss: 0.2845 - val_acc: 0.9212\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2903 - acc: 0.9191 - val_loss: 0.2843 - val_acc: 0.9212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2900 - acc: 0.9194 - val_loss: 0.2842 - val_acc: 0.9213\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2897 - acc: 0.9191 - val_loss: 0.2840 - val_acc: 0.9214\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2896 - acc: 0.9194 - val_loss: 0.2838 - val_acc: 0.9213\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2893 - acc: 0.9194 - val_loss: 0.2836 - val_acc: 0.9214\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2890 - acc: 0.9196 - val_loss: 0.2835 - val_acc: 0.9215\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2888 - acc: 0.9195 - val_loss: 0.2833 - val_acc: 0.9217\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2886 - acc: 0.9198 - val_loss: 0.2832 - val_acc: 0.9221\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2883 - acc: 0.9196 - val_loss: 0.2830 - val_acc: 0.9223\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2881 - acc: 0.9198 - val_loss: 0.2829 - val_acc: 0.9216\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2879 - acc: 0.9199 - val_loss: 0.2827 - val_acc: 0.9218\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2877 - acc: 0.9200 - val_loss: 0.2826 - val_acc: 0.9222\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2874 - acc: 0.9200 - val_loss: 0.2825 - val_acc: 0.9219\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2872 - acc: 0.9201 - val_loss: 0.2823 - val_acc: 0.9221\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2870 - acc: 0.9199 - val_loss: 0.2821 - val_acc: 0.9223\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2868 - acc: 0.9202 - val_loss: 0.2819 - val_acc: 0.9227\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.2866 - acc: 0.9201 - val_loss: 0.2819 - val_acc: 0.9220\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2864 - acc: 0.9202 - val_loss: 0.2817 - val_acc: 0.9219\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2862 - acc: 0.9202 - val_loss: 0.2816 - val_acc: 0.9224\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2860 - acc: 0.9202 - val_loss: 0.2814 - val_acc: 0.9223\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2857 - acc: 0.9205 - val_loss: 0.2813 - val_acc: 0.9223\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2856 - acc: 0.9205 - val_loss: 0.2812 - val_acc: 0.9222\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2854 - acc: 0.9206 - val_loss: 0.2811 - val_acc: 0.9222\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2852 - acc: 0.9207 - val_loss: 0.2809 - val_acc: 0.9224\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2850 - acc: 0.9210 - val_loss: 0.2808 - val_acc: 0.9223\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2848 - acc: 0.9206 - val_loss: 0.2807 - val_acc: 0.9222\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2846 - acc: 0.9208 - val_loss: 0.2806 - val_acc: 0.9225\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2844 - acc: 0.9210 - val_loss: 0.2804 - val_acc: 0.9225\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2842 - acc: 0.9206 - val_loss: 0.2804 - val_acc: 0.9225\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2840 - acc: 0.9213 - val_loss: 0.2803 - val_acc: 0.9222\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2839 - acc: 0.9209 - val_loss: 0.2801 - val_acc: 0.9228\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2836 - acc: 0.9210 - val_loss: 0.2800 - val_acc: 0.9222\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2835 - acc: 0.9210 - val_loss: 0.2798 - val_acc: 0.9228\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2833 - acc: 0.9213 - val_loss: 0.2796 - val_acc: 0.9225\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2831 - acc: 0.9216 - val_loss: 0.2796 - val_acc: 0.9225\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2829 - acc: 0.9216 - val_loss: 0.2794 - val_acc: 0.9228\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2828 - acc: 0.9213 - val_loss: 0.2793 - val_acc: 0.9227\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2826 - acc: 0.9214 - val_loss: 0.2792 - val_acc: 0.9229\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2824 - acc: 0.9214 - val_loss: 0.2792 - val_acc: 0.9227\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2822 - acc: 0.9218 - val_loss: 0.2791 - val_acc: 0.9227\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.2821 - acc: 0.9218 - val_loss: 0.2790 - val_acc: 0.9232\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2819 - acc: 0.9217 - val_loss: 0.2788 - val_acc: 0.9231\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.2817 - acc: 0.9219 - val_loss: 0.2788 - val_acc: 0.9232\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2816 - acc: 0.9217 - val_loss: 0.2786 - val_acc: 0.9232\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2814 - acc: 0.9221 - val_loss: 0.2785 - val_acc: 0.9232\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2812 - acc: 0.9219 - val_loss: 0.2784 - val_acc: 0.9235\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.2811 - acc: 0.9220 - val_loss: 0.2783 - val_acc: 0.9227\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2810 - acc: 0.9220 - val_loss: 0.2782 - val_acc: 0.9231\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2808 - acc: 0.9221 - val_loss: 0.2781 - val_acc: 0.9232\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2806 - acc: 0.9223 - val_loss: 0.2780 - val_acc: 0.9228\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2805 - acc: 0.9222 - val_loss: 0.2779 - val_acc: 0.9230\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2803 - acc: 0.9223 - val_loss: 0.2778 - val_acc: 0.9229\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2802 - acc: 0.9224 - val_loss: 0.2778 - val_acc: 0.9228\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2800 - acc: 0.9223 - val_loss: 0.2776 - val_acc: 0.9230\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2799 - acc: 0.9223 - val_loss: 0.2775 - val_acc: 0.9237\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2797 - acc: 0.9226 - val_loss: 0.2775 - val_acc: 0.9235\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2796 - acc: 0.9225 - val_loss: 0.2774 - val_acc: 0.9236\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2794 - acc: 0.9224 - val_loss: 0.2774 - val_acc: 0.9236\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2793 - acc: 0.9225 - val_loss: 0.2772 - val_acc: 0.9233\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2791 - acc: 0.9226 - val_loss: 0.2771 - val_acc: 0.9234\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2790 - acc: 0.9225 - val_loss: 0.2770 - val_acc: 0.9242\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2788 - acc: 0.9228 - val_loss: 0.2770 - val_acc: 0.9236\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2787 - acc: 0.9228 - val_loss: 0.2769 - val_acc: 0.9237\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2786 - acc: 0.9228 - val_loss: 0.2768 - val_acc: 0.9235\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2784 - acc: 0.9227 - val_loss: 0.2767 - val_acc: 0.9237\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2783 - acc: 0.9229 - val_loss: 0.2766 - val_acc: 0.9237\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2781 - acc: 0.9231 - val_loss: 0.2766 - val_acc: 0.9233\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2780 - acc: 0.9228 - val_loss: 0.2765 - val_acc: 0.9237\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2779 - acc: 0.9229 - val_loss: 0.2764 - val_acc: 0.9234\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2778 - acc: 0.9231 - val_loss: 0.2763 - val_acc: 0.9237\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2776 - acc: 0.9231 - val_loss: 0.2763 - val_acc: 0.9233\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2775 - acc: 0.9233 - val_loss: 0.2762 - val_acc: 0.9242\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2773 - acc: 0.9234 - val_loss: 0.2761 - val_acc: 0.9237\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2772 - acc: 0.9232 - val_loss: 0.2760 - val_acc: 0.9241\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2770 - acc: 0.9233 - val_loss: 0.2760 - val_acc: 0.9237\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2770 - acc: 0.9234 - val_loss: 0.2759 - val_acc: 0.9234\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2768 - acc: 0.9234 - val_loss: 0.2758 - val_acc: 0.9236\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2767 - acc: 0.9234 - val_loss: 0.2758 - val_acc: 0.9235\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2766 - acc: 0.9235 - val_loss: 0.2757 - val_acc: 0.9234\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2765 - acc: 0.9235 - val_loss: 0.2756 - val_acc: 0.9238\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2763 - acc: 0.9236 - val_loss: 0.2756 - val_acc: 0.9232\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2762 - acc: 0.9235 - val_loss: 0.2755 - val_acc: 0.9229\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2761 - acc: 0.9234 - val_loss: 0.2753 - val_acc: 0.9232\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.2760 - acc: 0.9239 - val_loss: 0.2754 - val_acc: 0.9233\n"
     ]
    }
   ],
   "source": [
    "#model fit\n",
    "history = model.fit(X_train, Y_train,\n",
    "                   batch_size= BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                   verbose=VERBOSE, validation_split = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 20us/step\n",
      "Test score : 0.277081135687232\n",
      "Test accuracy :  0.9233\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "score =model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score :\", score[0])\n",
    "print(\"Test accuracy : \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a baseline accuracy of 92.36% on training, 92.27% on validation, and 92.22% on the test. This is a good starting point, but we can certainly improve it. Let's see how.\n",
    "\n",
    "## Improving the simple net in Keras with hidden layers\n",
    "A first improvement is to add additional layers to our network. So, after the input layer, we have a first dense layer with the N_HIDDEN neurons and an activation function relu.\n",
    "\n",
    "After the first hidden layer, we have a second hidden layer, again with the N_HIDDEN neurons, followed by an output layer with 10 neurons, each of which will fire when the relative digit is recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 1.4828 - acc: 0.6230 - val_loss: 0.7583 - val_acc: 0.8285\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.6049 - acc: 0.8463 - val_loss: 0.4550 - val_acc: 0.8853\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.4398 - acc: 0.8801 - val_loss: 0.3709 - val_acc: 0.9020\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.3767 - acc: 0.8952 - val_loss: 0.3322 - val_acc: 0.9080\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.3415 - acc: 0.9026 - val_loss: 0.3055 - val_acc: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3175 - acc: 0.9085 - val_loss: 0.2880 - val_acc: 0.9184\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2989 - acc: 0.9136 - val_loss: 0.2727 - val_acc: 0.9222\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.2839 - acc: 0.9180 - val_loss: 0.2607 - val_acc: 0.9264\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.2714 - acc: 0.9217 - val_loss: 0.2504 - val_acc: 0.9297\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.2602 - acc: 0.9252 - val_loss: 0.2430 - val_acc: 0.9309\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.2502 - acc: 0.9285 - val_loss: 0.2341 - val_acc: 0.9333\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.2409 - acc: 0.9302 - val_loss: 0.2272 - val_acc: 0.9353\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.2326 - acc: 0.9334 - val_loss: 0.2228 - val_acc: 0.9366\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2253 - acc: 0.9354 - val_loss: 0.2147 - val_acc: 0.9395\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.2182 - acc: 0.9375 - val_loss: 0.2083 - val_acc: 0.9412\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.2116 - acc: 0.9392 - val_loss: 0.2030 - val_acc: 0.9432\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.2055 - acc: 0.9413 - val_loss: 0.1981 - val_acc: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.1996 - acc: 0.9429 - val_loss: 0.1932 - val_acc: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.1942 - acc: 0.9432 - val_loss: 0.1895 - val_acc: 0.9466\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1891 - acc: 0.9455 - val_loss: 0.1850 - val_acc: 0.9496\n",
      "10000/10000 [==============================] - 0s 33us/step\n",
      "Test score: 0.18604150784313678\n",
      "Test accuracy: 0.9461\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the code and see which result we get with this multilayer network. Not bad. By adding two hidden layers, we reached 94.50% on the training set, 94.63% on validation, and 94.41% on the test. This means that we gained an additional 2.2% accuracy on the test with respect to the previous network. However, we dramatically reduced the number of iterations from 200 to 20. That's good, but we want more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improving the simple net in Keras with dropout\n",
    "Now our baseline is 94.50% on the training set, 94.63% on validation, and 94.41% on the test.\n",
    "We decide to randomly drop with the dropout probability some of the values propagated inside our internal dense network of hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/250\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 1.7402 - acc: 0.4540 - val_loss: 0.9287 - val_acc: 0.8123\n",
      "Epoch 2/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.9229 - acc: 0.7230 - val_loss: 0.5398 - val_acc: 0.8652\n",
      "Epoch 3/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.6934 - acc: 0.7881 - val_loss: 0.4297 - val_acc: 0.8883\n",
      "Epoch 4/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.5947 - acc: 0.8210 - val_loss: 0.3789 - val_acc: 0.8977\n",
      "Epoch 5/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.5347 - acc: 0.8392 - val_loss: 0.3455 - val_acc: 0.9040\n",
      "Epoch 6/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.4976 - acc: 0.8524 - val_loss: 0.3232 - val_acc: 0.9105\n",
      "Epoch 7/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.4616 - acc: 0.8628 - val_loss: 0.3048 - val_acc: 0.9127\n",
      "Epoch 8/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.4386 - acc: 0.8687 - val_loss: 0.2895 - val_acc: 0.9175\n",
      "Epoch 9/250\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.4181 - acc: 0.8761 - val_loss: 0.2776 - val_acc: 0.9200\n",
      "Epoch 10/250\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.3990 - acc: 0.8838 - val_loss: 0.2656 - val_acc: 0.9234\n",
      "Epoch 11/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.3819 - acc: 0.8877 - val_loss: 0.2551 - val_acc: 0.9257\n",
      "Epoch 12/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.3688 - acc: 0.8920 - val_loss: 0.2465 - val_acc: 0.9283\n",
      "Epoch 13/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3571 - acc: 0.8944 - val_loss: 0.2388 - val_acc: 0.9300\n",
      "Epoch 14/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.3466 - acc: 0.8992 - val_loss: 0.2319 - val_acc: 0.9325\n",
      "Epoch 15/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3358 - acc: 0.9016 - val_loss: 0.2261 - val_acc: 0.9339\n",
      "Epoch 16/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.3244 - acc: 0.9055 - val_loss: 0.2180 - val_acc: 0.9352\n",
      "Epoch 17/250\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.3142 - acc: 0.9085 - val_loss: 0.2121 - val_acc: 0.9376\n",
      "Epoch 18/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3102 - acc: 0.9094 - val_loss: 0.2076 - val_acc: 0.9389\n",
      "Epoch 19/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.3018 - acc: 0.9118 - val_loss: 0.2018 - val_acc: 0.9410\n",
      "Epoch 20/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2931 - acc: 0.9131 - val_loss: 0.1974 - val_acc: 0.9422\n",
      "Epoch 21/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.2866 - acc: 0.9172 - val_loss: 0.1920 - val_acc: 0.9437\n",
      "Epoch 22/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.2789 - acc: 0.9173 - val_loss: 0.1878 - val_acc: 0.9446\n",
      "Epoch 23/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2730 - acc: 0.9200 - val_loss: 0.1841 - val_acc: 0.9464\n",
      "Epoch 24/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2686 - acc: 0.9211 - val_loss: 0.1811 - val_acc: 0.9466\n",
      "Epoch 25/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2618 - acc: 0.9235 - val_loss: 0.1770 - val_acc: 0.9479\n",
      "Epoch 26/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2584 - acc: 0.9249 - val_loss: 0.1736 - val_acc: 0.9489\n",
      "Epoch 27/250\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2539 - acc: 0.9252 - val_loss: 0.1706 - val_acc: 0.9496\n",
      "Epoch 28/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2453 - acc: 0.9276 - val_loss: 0.1677 - val_acc: 0.9502\n",
      "Epoch 29/250\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2427 - acc: 0.9275 - val_loss: 0.1640 - val_acc: 0.9518\n",
      "Epoch 30/250\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2397 - acc: 0.9297 - val_loss: 0.1616 - val_acc: 0.9520\n",
      "Epoch 31/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2361 - acc: 0.9305 - val_loss: 0.1590 - val_acc: 0.9533\n",
      "Epoch 32/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2320 - acc: 0.9305 - val_loss: 0.1568 - val_acc: 0.9545\n",
      "Epoch 33/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2284 - acc: 0.9326 - val_loss: 0.1534 - val_acc: 0.9551\n",
      "Epoch 34/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2257 - acc: 0.9327 - val_loss: 0.1519 - val_acc: 0.9550\n",
      "Epoch 35/250\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2215 - acc: 0.9353 - val_loss: 0.1501 - val_acc: 0.9557\n",
      "Epoch 36/250\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.2169 - acc: 0.9355 - val_loss: 0.1485 - val_acc: 0.9563\n",
      "Epoch 37/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2124 - acc: 0.9376 - val_loss: 0.1459 - val_acc: 0.9569\n",
      "Epoch 38/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2122 - acc: 0.9370 - val_loss: 0.1433 - val_acc: 0.9580\n",
      "Epoch 39/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2091 - acc: 0.9387 - val_loss: 0.1422 - val_acc: 0.9575\n",
      "Epoch 40/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2042 - acc: 0.9393 - val_loss: 0.1410 - val_acc: 0.9580\n",
      "Epoch 41/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2027 - acc: 0.9396 - val_loss: 0.1397 - val_acc: 0.9584\n",
      "Epoch 42/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.1985 - acc: 0.9417 - val_loss: 0.1367 - val_acc: 0.9593\n",
      "Epoch 43/250\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2003 - acc: 0.9408 - val_loss: 0.1350 - val_acc: 0.9607\n",
      "Epoch 44/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.1953 - acc: 0.9421 - val_loss: 0.1337 - val_acc: 0.9607\n",
      "Epoch 45/250\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.1921 - acc: 0.9432 - val_loss: 0.1332 - val_acc: 0.9600\n",
      "Epoch 46/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.1901 - acc: 0.9444 - val_loss: 0.1317 - val_acc: 0.9614\n",
      "Epoch 47/250\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.1877 - acc: 0.9448 - val_loss: 0.1300 - val_acc: 0.9612\n",
      "Epoch 48/250\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.1867 - acc: 0.9442 - val_loss: 0.1301 - val_acc: 0.9618\n",
      "Epoch 49/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.1865 - acc: 0.9452 - val_loss: 0.1283 - val_acc: 0.9613\n",
      "Epoch 50/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1803 - acc: 0.9462 - val_loss: 0.1268 - val_acc: 0.9621\n",
      "Epoch 51/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.1823 - acc: 0.9465 - val_loss: 0.1255 - val_acc: 0.9632\n",
      "Epoch 52/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.1795 - acc: 0.9459 - val_loss: 0.1245 - val_acc: 0.9632\n",
      "Epoch 53/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1753 - acc: 0.9479 - val_loss: 0.1234 - val_acc: 0.9635\n",
      "Epoch 54/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.1739 - acc: 0.9477 - val_loss: 0.1221 - val_acc: 0.9636\n",
      "Epoch 55/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1736 - acc: 0.9491 - val_loss: 0.1209 - val_acc: 0.9646\n",
      "Epoch 56/250\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.1720 - acc: 0.9486 - val_loss: 0.1208 - val_acc: 0.9638\n",
      "Epoch 57/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.1693 - acc: 0.9504 - val_loss: 0.1189 - val_acc: 0.9649\n",
      "Epoch 58/250\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.1664 - acc: 0.9508 - val_loss: 0.1188 - val_acc: 0.9648\n",
      "Epoch 59/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.1682 - acc: 0.9501 - val_loss: 0.1173 - val_acc: 0.9652\n",
      "Epoch 60/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1647 - acc: 0.9515 - val_loss: 0.1166 - val_acc: 0.9652\n",
      "Epoch 61/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.1615 - acc: 0.9522 - val_loss: 0.1157 - val_acc: 0.9655\n",
      "Epoch 62/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1592 - acc: 0.9526 - val_loss: 0.1150 - val_acc: 0.9655\n",
      "Epoch 63/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.1587 - acc: 0.9533 - val_loss: 0.1142 - val_acc: 0.9658\n",
      "Epoch 64/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.1565 - acc: 0.9531 - val_loss: 0.1126 - val_acc: 0.9667\n",
      "Epoch 65/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.1561 - acc: 0.9540 - val_loss: 0.1128 - val_acc: 0.9668\n",
      "Epoch 66/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.1573 - acc: 0.9533 - val_loss: 0.1120 - val_acc: 0.9662\n",
      "Epoch 67/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1554 - acc: 0.9548 - val_loss: 0.1106 - val_acc: 0.9669\n",
      "Epoch 68/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1525 - acc: 0.9544 - val_loss: 0.1103 - val_acc: 0.9672\n",
      "Epoch 69/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1524 - acc: 0.9554 - val_loss: 0.1089 - val_acc: 0.9676\n",
      "Epoch 70/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1503 - acc: 0.9552 - val_loss: 0.1086 - val_acc: 0.9677\n",
      "Epoch 71/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1479 - acc: 0.9566 - val_loss: 0.1082 - val_acc: 0.9681\n",
      "Epoch 72/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1451 - acc: 0.9567 - val_loss: 0.1072 - val_acc: 0.9687\n",
      "Epoch 73/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1462 - acc: 0.9568 - val_loss: 0.1068 - val_acc: 0.9681\n",
      "Epoch 74/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1439 - acc: 0.9584 - val_loss: 0.1067 - val_acc: 0.9682\n",
      "Epoch 75/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1447 - acc: 0.9566 - val_loss: 0.1059 - val_acc: 0.9682\n",
      "Epoch 76/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1414 - acc: 0.9580 - val_loss: 0.1059 - val_acc: 0.9685\n",
      "Epoch 77/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.1421 - acc: 0.9578 - val_loss: 0.1055 - val_acc: 0.9681\n",
      "Epoch 78/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.1399 - acc: 0.9589 - val_loss: 0.1044 - val_acc: 0.9690\n",
      "Epoch 79/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1416 - acc: 0.9573 - val_loss: 0.1042 - val_acc: 0.9687\n",
      "Epoch 80/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.1393 - acc: 0.9594 - val_loss: 0.1034 - val_acc: 0.9689\n",
      "Epoch 81/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.1371 - acc: 0.9592 - val_loss: 0.1035 - val_acc: 0.9688\n",
      "Epoch 82/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1366 - acc: 0.9578 - val_loss: 0.1031 - val_acc: 0.9689\n",
      "Epoch 83/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.1345 - acc: 0.9596 - val_loss: 0.1020 - val_acc: 0.9693\n",
      "Epoch 84/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1338 - acc: 0.9600 - val_loss: 0.1015 - val_acc: 0.9692\n",
      "Epoch 85/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1337 - acc: 0.9602 - val_loss: 0.1015 - val_acc: 0.9695\n",
      "Epoch 86/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1347 - acc: 0.9600 - val_loss: 0.1006 - val_acc: 0.9697\n",
      "Epoch 87/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1305 - acc: 0.9608 - val_loss: 0.1004 - val_acc: 0.9704\n",
      "Epoch 88/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1322 - acc: 0.9596 - val_loss: 0.1000 - val_acc: 0.9697\n",
      "Epoch 89/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.1305 - acc: 0.9608 - val_loss: 0.0991 - val_acc: 0.9701\n",
      "Epoch 90/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1321 - acc: 0.9603 - val_loss: 0.0987 - val_acc: 0.9705\n",
      "Epoch 91/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1286 - acc: 0.9622 - val_loss: 0.0982 - val_acc: 0.9708\n",
      "Epoch 92/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.1318 - acc: 0.9601 - val_loss: 0.0986 - val_acc: 0.9716\n",
      "Epoch 93/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1285 - acc: 0.9615 - val_loss: 0.0977 - val_acc: 0.9710\n",
      "Epoch 94/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1250 - acc: 0.9621 - val_loss: 0.0975 - val_acc: 0.9712\n",
      "Epoch 95/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1265 - acc: 0.9626 - val_loss: 0.0974 - val_acc: 0.9716\n",
      "Epoch 96/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1239 - acc: 0.9625 - val_loss: 0.0969 - val_acc: 0.9716\n",
      "Epoch 97/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1241 - acc: 0.9622 - val_loss: 0.0960 - val_acc: 0.9712\n",
      "Epoch 98/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1235 - acc: 0.9631 - val_loss: 0.0965 - val_acc: 0.9716\n",
      "Epoch 99/250\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.1217 - acc: 0.9643 - val_loss: 0.0957 - val_acc: 0.9717\n",
      "Epoch 100/250\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.1212 - acc: 0.9635 - val_loss: 0.0957 - val_acc: 0.9721\n",
      "Epoch 101/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1228 - acc: 0.9631 - val_loss: 0.0961 - val_acc: 0.9722\n",
      "Epoch 102/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1215 - acc: 0.9641 - val_loss: 0.0947 - val_acc: 0.9722\n",
      "Epoch 103/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1193 - acc: 0.9647 - val_loss: 0.0950 - val_acc: 0.9718\n",
      "Epoch 104/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1178 - acc: 0.9648 - val_loss: 0.0941 - val_acc: 0.9723\n",
      "Epoch 105/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.1164 - acc: 0.9654 - val_loss: 0.0943 - val_acc: 0.9730\n",
      "Epoch 106/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1170 - acc: 0.9648 - val_loss: 0.0939 - val_acc: 0.9727\n",
      "Epoch 107/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1169 - acc: 0.9647 - val_loss: 0.0940 - val_acc: 0.9730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.1139 - acc: 0.9664 - val_loss: 0.0933 - val_acc: 0.9726\n",
      "Epoch 109/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1146 - acc: 0.9658 - val_loss: 0.0933 - val_acc: 0.9733\n",
      "Epoch 110/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.1141 - acc: 0.9659 - val_loss: 0.0928 - val_acc: 0.9727\n",
      "Epoch 111/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1146 - acc: 0.9658 - val_loss: 0.0927 - val_acc: 0.9724\n",
      "Epoch 112/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1117 - acc: 0.9662 - val_loss: 0.0917 - val_acc: 0.9739\n",
      "Epoch 113/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1127 - acc: 0.9658 - val_loss: 0.0921 - val_acc: 0.9732\n",
      "Epoch 114/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1145 - acc: 0.9655 - val_loss: 0.0914 - val_acc: 0.9736\n",
      "Epoch 115/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.1114 - acc: 0.9662 - val_loss: 0.0913 - val_acc: 0.9741\n",
      "Epoch 116/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1088 - acc: 0.9671 - val_loss: 0.0911 - val_acc: 0.9739\n",
      "Epoch 117/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1116 - acc: 0.9665 - val_loss: 0.0912 - val_acc: 0.9737\n",
      "Epoch 118/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1087 - acc: 0.9671 - val_loss: 0.0907 - val_acc: 0.9737\n",
      "Epoch 119/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1117 - acc: 0.9658 - val_loss: 0.0910 - val_acc: 0.9742\n",
      "Epoch 120/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1071 - acc: 0.9676 - val_loss: 0.0901 - val_acc: 0.9743\n",
      "Epoch 121/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1084 - acc: 0.9670 - val_loss: 0.0904 - val_acc: 0.9742\n",
      "Epoch 122/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.1074 - acc: 0.9672 - val_loss: 0.0895 - val_acc: 0.9745\n",
      "Epoch 123/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1043 - acc: 0.9679 - val_loss: 0.0891 - val_acc: 0.9747\n",
      "Epoch 124/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1047 - acc: 0.9683 - val_loss: 0.0894 - val_acc: 0.9743\n",
      "Epoch 125/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1043 - acc: 0.9690 - val_loss: 0.0893 - val_acc: 0.9742\n",
      "Epoch 126/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.1035 - acc: 0.9686 - val_loss: 0.0888 - val_acc: 0.9746\n",
      "Epoch 127/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.1033 - acc: 0.9684 - val_loss: 0.0889 - val_acc: 0.9746\n",
      "Epoch 128/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.1043 - acc: 0.9687 - val_loss: 0.0883 - val_acc: 0.9751\n",
      "Epoch 129/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1051 - acc: 0.9678 - val_loss: 0.0882 - val_acc: 0.9751\n",
      "Epoch 130/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1040 - acc: 0.9689 - val_loss: 0.0883 - val_acc: 0.9752\n",
      "Epoch 131/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1026 - acc: 0.9689 - val_loss: 0.0876 - val_acc: 0.9749\n",
      "Epoch 132/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0999 - acc: 0.9702 - val_loss: 0.0879 - val_acc: 0.9749\n",
      "Epoch 133/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.1009 - acc: 0.9687 - val_loss: 0.0876 - val_acc: 0.9752\n",
      "Epoch 134/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0989 - acc: 0.9688 - val_loss: 0.0877 - val_acc: 0.9747\n",
      "Epoch 135/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1009 - acc: 0.9693 - val_loss: 0.0879 - val_acc: 0.9749\n",
      "Epoch 136/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.1001 - acc: 0.9705 - val_loss: 0.0875 - val_acc: 0.9752\n",
      "Epoch 137/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0996 - acc: 0.9694 - val_loss: 0.0878 - val_acc: 0.9757\n",
      "Epoch 138/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1004 - acc: 0.9691 - val_loss: 0.0875 - val_acc: 0.9755\n",
      "Epoch 139/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0975 - acc: 0.9705 - val_loss: 0.0873 - val_acc: 0.9754\n",
      "Epoch 140/250\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0965 - acc: 0.9709 - val_loss: 0.0869 - val_acc: 0.9757\n",
      "Epoch 141/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0971 - acc: 0.9699 - val_loss: 0.0867 - val_acc: 0.9761\n",
      "Epoch 142/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0953 - acc: 0.9708 - val_loss: 0.0864 - val_acc: 0.9762\n",
      "Epoch 143/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0969 - acc: 0.9703 - val_loss: 0.0868 - val_acc: 0.9759\n",
      "Epoch 144/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0946 - acc: 0.9710 - val_loss: 0.0864 - val_acc: 0.9758\n",
      "Epoch 145/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0962 - acc: 0.9706 - val_loss: 0.0859 - val_acc: 0.9759\n",
      "Epoch 146/250\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0939 - acc: 0.9720 - val_loss: 0.0863 - val_acc: 0.9756\n",
      "Epoch 147/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0937 - acc: 0.9712 - val_loss: 0.0865 - val_acc: 0.9762\n",
      "Epoch 148/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0949 - acc: 0.9710 - val_loss: 0.0861 - val_acc: 0.9758\n",
      "Epoch 149/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0925 - acc: 0.9721 - val_loss: 0.0856 - val_acc: 0.9758\n",
      "Epoch 150/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0917 - acc: 0.9719 - val_loss: 0.0862 - val_acc: 0.9757\n",
      "Epoch 151/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0942 - acc: 0.9719 - val_loss: 0.0856 - val_acc: 0.9762\n",
      "Epoch 152/250\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0923 - acc: 0.9723 - val_loss: 0.0853 - val_acc: 0.9762\n",
      "Epoch 153/250\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0892 - acc: 0.9727 - val_loss: 0.0852 - val_acc: 0.9759\n",
      "Epoch 154/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0916 - acc: 0.9724 - val_loss: 0.0854 - val_acc: 0.9761\n",
      "Epoch 155/250\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0909 - acc: 0.9725 - val_loss: 0.0850 - val_acc: 0.9760\n",
      "Epoch 156/250\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.0911 - acc: 0.9726 - val_loss: 0.0849 - val_acc: 0.9759\n",
      "Epoch 157/250\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0901 - acc: 0.9731 - val_loss: 0.0849 - val_acc: 0.9762\n",
      "Epoch 158/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0886 - acc: 0.9731 - val_loss: 0.0854 - val_acc: 0.9761\n",
      "Epoch 159/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0878 - acc: 0.9728 - val_loss: 0.0846 - val_acc: 0.9762\n",
      "Epoch 160/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0894 - acc: 0.9729 - val_loss: 0.0848 - val_acc: 0.9762\n",
      "Epoch 161/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0888 - acc: 0.9729 - val_loss: 0.0842 - val_acc: 0.9767\n",
      "Epoch 162/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0885 - acc: 0.9734 - val_loss: 0.0843 - val_acc: 0.9763\n",
      "Epoch 163/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0870 - acc: 0.9733 - val_loss: 0.0846 - val_acc: 0.9765\n",
      "Epoch 164/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0878 - acc: 0.9730 - val_loss: 0.0841 - val_acc: 0.9767\n",
      "Epoch 165/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0860 - acc: 0.9735 - val_loss: 0.0839 - val_acc: 0.9764\n",
      "Epoch 166/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0863 - acc: 0.9731 - val_loss: 0.0848 - val_acc: 0.9764\n",
      "Epoch 167/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0858 - acc: 0.9738 - val_loss: 0.0847 - val_acc: 0.9761\n",
      "Epoch 168/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0838 - acc: 0.9747 - val_loss: 0.0843 - val_acc: 0.9761\n",
      "Epoch 169/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0854 - acc: 0.9739 - val_loss: 0.0840 - val_acc: 0.9760\n",
      "Epoch 170/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0872 - acc: 0.9735 - val_loss: 0.0834 - val_acc: 0.9762\n",
      "Epoch 171/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0854 - acc: 0.9739 - val_loss: 0.0831 - val_acc: 0.9766\n",
      "Epoch 172/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0846 - acc: 0.9737 - val_loss: 0.0833 - val_acc: 0.9763\n",
      "Epoch 173/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0858 - acc: 0.9739 - val_loss: 0.0840 - val_acc: 0.9764\n",
      "Epoch 174/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0826 - acc: 0.9747 - val_loss: 0.0833 - val_acc: 0.9762\n",
      "Epoch 175/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0857 - acc: 0.9733 - val_loss: 0.0836 - val_acc: 0.9764\n",
      "Epoch 176/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0807 - acc: 0.9754 - val_loss: 0.0837 - val_acc: 0.9765\n",
      "Epoch 177/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0823 - acc: 0.9751 - val_loss: 0.0828 - val_acc: 0.9771\n",
      "Epoch 178/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0825 - acc: 0.9747 - val_loss: 0.0827 - val_acc: 0.9772\n",
      "Epoch 179/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0812 - acc: 0.9752 - val_loss: 0.0822 - val_acc: 0.9764\n",
      "Epoch 180/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0835 - acc: 0.9741 - val_loss: 0.0831 - val_acc: 0.9768\n",
      "Epoch 181/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0812 - acc: 0.9750 - val_loss: 0.0818 - val_acc: 0.9767\n",
      "Epoch 182/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0785 - acc: 0.9756 - val_loss: 0.0826 - val_acc: 0.9770\n",
      "Epoch 183/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0803 - acc: 0.9758 - val_loss: 0.0824 - val_acc: 0.9767\n",
      "Epoch 184/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0808 - acc: 0.9751 - val_loss: 0.0818 - val_acc: 0.9766\n",
      "Epoch 185/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0792 - acc: 0.9755 - val_loss: 0.0821 - val_acc: 0.9765\n",
      "Epoch 186/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0807 - acc: 0.9757 - val_loss: 0.0824 - val_acc: 0.9768\n",
      "Epoch 187/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0785 - acc: 0.9756 - val_loss: 0.0820 - val_acc: 0.9762\n",
      "Epoch 188/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0795 - acc: 0.9754 - val_loss: 0.0813 - val_acc: 0.9768\n",
      "Epoch 189/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0775 - acc: 0.9757 - val_loss: 0.0820 - val_acc: 0.9767\n",
      "Epoch 190/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0776 - acc: 0.9759 - val_loss: 0.0821 - val_acc: 0.9767\n",
      "Epoch 191/250\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0781 - acc: 0.9767 - val_loss: 0.0822 - val_acc: 0.9762\n",
      "Epoch 192/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0797 - acc: 0.9758 - val_loss: 0.0821 - val_acc: 0.9772\n",
      "Epoch 193/250\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0776 - acc: 0.9760 - val_loss: 0.0818 - val_acc: 0.9768\n",
      "Epoch 194/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0780 - acc: 0.9762 - val_loss: 0.0818 - val_acc: 0.9770\n",
      "Epoch 195/250\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0768 - acc: 0.9761 - val_loss: 0.0820 - val_acc: 0.9764\n",
      "Epoch 196/250\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.0742 - acc: 0.9770 - val_loss: 0.0818 - val_acc: 0.9765\n",
      "Epoch 197/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0770 - acc: 0.9756 - val_loss: 0.0816 - val_acc: 0.9765\n",
      "Epoch 198/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0749 - acc: 0.9769 - val_loss: 0.0814 - val_acc: 0.9768\n",
      "Epoch 199/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0750 - acc: 0.9770 - val_loss: 0.0811 - val_acc: 0.9769\n",
      "Epoch 200/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0743 - acc: 0.9767 - val_loss: 0.0809 - val_acc: 0.9765\n",
      "Epoch 201/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0746 - acc: 0.9762 - val_loss: 0.0814 - val_acc: 0.9767\n",
      "Epoch 202/250\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0782 - acc: 0.9762 - val_loss: 0.0811 - val_acc: 0.9774\n",
      "Epoch 203/250\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0749 - acc: 0.9771 - val_loss: 0.0810 - val_acc: 0.9768\n",
      "Epoch 204/250\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0719 - acc: 0.9772 - val_loss: 0.0811 - val_acc: 0.9771\n",
      "Epoch 205/250\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0752 - acc: 0.9768 - val_loss: 0.0814 - val_acc: 0.9769\n",
      "Epoch 206/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0731 - acc: 0.9774 - val_loss: 0.0812 - val_acc: 0.9771\n",
      "Epoch 207/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0724 - acc: 0.9774 - val_loss: 0.0811 - val_acc: 0.9762\n",
      "Epoch 208/250\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0725 - acc: 0.9771 - val_loss: 0.0814 - val_acc: 0.9769\n",
      "Epoch 209/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0732 - acc: 0.9775 - val_loss: 0.0814 - val_acc: 0.9768\n",
      "Epoch 210/250\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0711 - acc: 0.9779 - val_loss: 0.0816 - val_acc: 0.9767\n",
      "Epoch 211/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0734 - acc: 0.9765 - val_loss: 0.0819 - val_acc: 0.9772\n",
      "Epoch 212/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0739 - acc: 0.9770 - val_loss: 0.0815 - val_acc: 0.9768\n",
      "Epoch 213/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0724 - acc: 0.9779 - val_loss: 0.0809 - val_acc: 0.9776\n",
      "Epoch 214/250\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0722 - acc: 0.9780 - val_loss: 0.0811 - val_acc: 0.9775\n",
      "Epoch 215/250\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0706 - acc: 0.9784 - val_loss: 0.0808 - val_acc: 0.9772\n",
      "Epoch 216/250\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0708 - acc: 0.9776 - val_loss: 0.0809 - val_acc: 0.9772\n",
      "Epoch 217/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0708 - acc: 0.9784 - val_loss: 0.0806 - val_acc: 0.9771\n",
      "Epoch 218/250\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.0687 - acc: 0.9786 - val_loss: 0.0809 - val_acc: 0.9769\n",
      "Epoch 219/250\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.0707 - acc: 0.9779 - val_loss: 0.0802 - val_acc: 0.9771\n",
      "Epoch 220/250\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0689 - acc: 0.9790 - val_loss: 0.0803 - val_acc: 0.9773\n",
      "Epoch 221/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0705 - acc: 0.9778 - val_loss: 0.0811 - val_acc: 0.9769\n",
      "Epoch 222/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0687 - acc: 0.9788 - val_loss: 0.0807 - val_acc: 0.9767\n",
      "Epoch 223/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0708 - acc: 0.9773 - val_loss: 0.0806 - val_acc: 0.9772\n",
      "Epoch 224/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0700 - acc: 0.9790 - val_loss: 0.0804 - val_acc: 0.9772\n",
      "Epoch 225/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0674 - acc: 0.9791 - val_loss: 0.0806 - val_acc: 0.9776\n",
      "Epoch 226/250\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0684 - acc: 0.9788 - val_loss: 0.0805 - val_acc: 0.9770\n",
      "Epoch 227/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0668 - acc: 0.9793 - val_loss: 0.0808 - val_acc: 0.9772\n",
      "Epoch 228/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0693 - acc: 0.9786 - val_loss: 0.0806 - val_acc: 0.9769\n",
      "Epoch 229/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0688 - acc: 0.9785 - val_loss: 0.0806 - val_acc: 0.9770\n",
      "Epoch 230/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0685 - acc: 0.9786 - val_loss: 0.0801 - val_acc: 0.9769\n",
      "Epoch 231/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0672 - acc: 0.9793 - val_loss: 0.0799 - val_acc: 0.9772\n",
      "Epoch 232/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0681 - acc: 0.9785 - val_loss: 0.0802 - val_acc: 0.9772\n",
      "Epoch 233/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0648 - acc: 0.9797 - val_loss: 0.0805 - val_acc: 0.9774\n",
      "Epoch 234/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0679 - acc: 0.9782 - val_loss: 0.0802 - val_acc: 0.9777\n",
      "Epoch 235/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0664 - acc: 0.9792 - val_loss: 0.0795 - val_acc: 0.9774\n",
      "Epoch 236/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0687 - acc: 0.9785 - val_loss: 0.0795 - val_acc: 0.9773\n",
      "Epoch 237/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0651 - acc: 0.9798 - val_loss: 0.0800 - val_acc: 0.9777\n",
      "Epoch 238/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0666 - acc: 0.9793 - val_loss: 0.0803 - val_acc: 0.9775\n",
      "Epoch 239/250\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0669 - acc: 0.9793 - val_loss: 0.0807 - val_acc: 0.9776\n",
      "Epoch 240/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0688 - acc: 0.9788 - val_loss: 0.0803 - val_acc: 0.9776\n",
      "Epoch 241/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0648 - acc: 0.9798 - val_loss: 0.0807 - val_acc: 0.9774\n",
      "Epoch 242/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0657 - acc: 0.9788 - val_loss: 0.0798 - val_acc: 0.9777\n",
      "Epoch 243/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0658 - acc: 0.9793 - val_loss: 0.0796 - val_acc: 0.9778\n",
      "Epoch 244/250\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0656 - acc: 0.9797 - val_loss: 0.0798 - val_acc: 0.9777\n",
      "Epoch 245/250\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0635 - acc: 0.9805 - val_loss: 0.0799 - val_acc: 0.9774\n",
      "Epoch 246/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0628 - acc: 0.9803 - val_loss: 0.0810 - val_acc: 0.9770\n",
      "Epoch 247/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0621 - acc: 0.9804 - val_loss: 0.0801 - val_acc: 0.9773\n",
      "Epoch 248/250\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0633 - acc: 0.9804 - val_loss: 0.0802 - val_acc: 0.9769\n",
      "Epoch 249/250\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0636 - acc: 0.9800 - val_loss: 0.0804 - val_acc: 0.9777\n",
      "Epoch 250/250\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0617 - acc: 0.9808 - val_loss: 0.0801 - val_acc: 0.9778\n",
      "10000/10000 [==============================] - 0s 43us/step\n",
      "Test score: 0.07727144756572088\n",
      "Test accuracy: 0.9777\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 250\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try to increase significantly the number of epochs up to 250, and we get 98.1% accuracy on training, 97.73% on validation, and 97.7% on the test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RMSprop optimizer on 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.4775 - acc: 0.8574 - val_loss: 0.1862 - val_acc: 0.9449\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2266 - acc: 0.9322 - val_loss: 0.1379 - val_acc: 0.9593\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.1753 - acc: 0.9475 - val_loss: 0.1159 - val_acc: 0.9653\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.1511 - acc: 0.9546 - val_loss: 0.1166 - val_acc: 0.9655\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.1353 - acc: 0.9604 - val_loss: 0.1043 - val_acc: 0.9710\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.1237 - acc: 0.9628 - val_loss: 0.0997 - val_acc: 0.9721\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.1134 - acc: 0.9659 - val_loss: 0.0993 - val_acc: 0.9722\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.1059 - acc: 0.9687 - val_loss: 0.1014 - val_acc: 0.9713\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0997 - acc: 0.9698 - val_loss: 0.0977 - val_acc: 0.9743\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0935 - acc: 0.9723 - val_loss: 0.0991 - val_acc: 0.9745\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0895 - acc: 0.9731 - val_loss: 0.0962 - val_acc: 0.9757\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0874 - acc: 0.9744 - val_loss: 0.1001 - val_acc: 0.9741\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0865 - acc: 0.9744 - val_loss: 0.0970 - val_acc: 0.9747\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0794 - acc: 0.9763 - val_loss: 0.1016 - val_acc: 0.9762\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0806 - acc: 0.9762 - val_loss: 0.0963 - val_acc: 0.9769\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0756 - acc: 0.9773 - val_loss: 0.0984 - val_acc: 0.9767\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0743 - acc: 0.9786 - val_loss: 0.1059 - val_acc: 0.9747\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0754 - acc: 0.9777 - val_loss: 0.1016 - val_acc: 0.9767\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0719 - acc: 0.9792 - val_loss: 0.1074 - val_acc: 0.9762\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0702 - acc: 0.9791 - val_loss: 0.1088 - val_acc: 0.9760\n",
      "10000/10000 [==============================] - 0s 33us/step\n",
      "Test score: 0.10223156950175762\n",
      "Test accuracy: 0.9777\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = RMSprop() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RMSprop optimizer on 20 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.5194 - acc: 0.8434 - val_loss: 0.1881 - val_acc: 0.9449\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2321 - acc: 0.9316 - val_loss: 0.1410 - val_acc: 0.9570\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.1801 - acc: 0.9463 - val_loss: 0.1153 - val_acc: 0.9656\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.1527 - acc: 0.9552 - val_loss: 0.1062 - val_acc: 0.9681\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.1313 - acc: 0.9598 - val_loss: 0.0991 - val_acc: 0.9712\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.1207 - acc: 0.9623 - val_loss: 0.0898 - val_acc: 0.9728\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.1077 - acc: 0.9666 - val_loss: 0.0886 - val_acc: 0.9729\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0999 - acc: 0.9688 - val_loss: 0.0845 - val_acc: 0.9752\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0914 - acc: 0.9720 - val_loss: 0.0847 - val_acc: 0.9750\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0849 - acc: 0.9736 - val_loss: 0.0805 - val_acc: 0.9773\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0804 - acc: 0.9741 - val_loss: 0.0864 - val_acc: 0.9755\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0779 - acc: 0.9745 - val_loss: 0.0817 - val_acc: 0.9774\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0726 - acc: 0.9769 - val_loss: 0.0837 - val_acc: 0.9748\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0694 - acc: 0.9783 - val_loss: 0.0859 - val_acc: 0.9770\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0680 - acc: 0.9775 - val_loss: 0.0809 - val_acc: 0.9775\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0640 - acc: 0.9791 - val_loss: 0.0806 - val_acc: 0.9761\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0596 - acc: 0.9800 - val_loss: 0.0829 - val_acc: 0.9774\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0580 - acc: 0.9806 - val_loss: 0.0826 - val_acc: 0.9776\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0587 - acc: 0.9809 - val_loss: 0.0846 - val_acc: 0.9763\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0575 - acc: 0.9813 - val_loss: 0.0815 - val_acc: 0.9788\n",
      "10000/10000 [==============================] - 0s 34us/step\n",
      "Test score: 0.07790470074359619\n",
      "Test accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = Adam() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.5188 - acc: 0.8437 - val_loss: 0.1884 - val_acc: 0.9438\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.2336 - acc: 0.9305 - val_loss: 0.1396 - val_acc: 0.9585\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.1804 - acc: 0.9463 - val_loss: 0.1138 - val_acc: 0.9661\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.1533 - acc: 0.9536 - val_loss: 0.1076 - val_acc: 0.9677\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.1321 - acc: 0.9593 - val_loss: 0.0995 - val_acc: 0.9704\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1194 - acc: 0.9631 - val_loss: 0.0878 - val_acc: 0.9735\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1061 - acc: 0.9676 - val_loss: 0.0906 - val_acc: 0.9720\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0998 - acc: 0.9698 - val_loss: 0.0864 - val_acc: 0.9744\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.0888 - acc: 0.9724 - val_loss: 0.0834 - val_acc: 0.9752\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.0863 - acc: 0.9736 - val_loss: 0.0812 - val_acc: 0.9769\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0803 - acc: 0.9734 - val_loss: 0.0837 - val_acc: 0.9763\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.0774 - acc: 0.9750 - val_loss: 0.0788 - val_acc: 0.9778\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.0720 - acc: 0.9767 - val_loss: 0.0804 - val_acc: 0.9766\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0680 - acc: 0.9784 - val_loss: 0.0824 - val_acc: 0.9776\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0687 - acc: 0.9778 - val_loss: 0.0804 - val_acc: 0.9787\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0645 - acc: 0.9786 - val_loss: 0.0822 - val_acc: 0.9785\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.0619 - acc: 0.9790 - val_loss: 0.0790 - val_acc: 0.9785\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0596 - acc: 0.9797 - val_loss: 0.0773 - val_acc: 0.9783\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0561 - acc: 0.9816 - val_loss: 0.0841 - val_acc: 0.9782\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0573 - acc: 0.9813 - val_loss: 0.0819 - val_acc: 0.9777\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Test score: 0.07579163712106092\n",
      "Test accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "MODEL_DIR = \"../models\"\n",
    "\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = Adam() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# save best model \n",
    "checkpoint = ModelCheckpoint( filepath=os.path.join(MODEL_DIR, \"model-{epoch:02d}.h5\")) \n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18237d2828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18284dcf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting output\n",
    "When a net is trained, it can be course be used for predictions. In Keras, this is very simple. We can use the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
